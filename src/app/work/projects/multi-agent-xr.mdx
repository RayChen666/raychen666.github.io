---
title: "XR-MultiAgent - The first system to explore collaborative agentic spatial intelligence in XR world"
publishedAt: "2025-11-09"
summary: "A real-time XR spatial intelligence system that integrates natural language command input 
with a multi-agent backend using FastAPI and LangGraph for agentic collaborative spatial reasoning, 
enabling dynamic, spatially coordinated XR interactions with scene objects, demonstrating scalable 
multi-modal input workflows and immersive interface behaviors (work aiming for IEEE VR 2027)."

images:
  - "/images/projects/project-01/multiagentxr-cover-image.jpg"
  - "/images/projects/project-01/multiagentxr-scene-image.jpg"
  - "/images/projects/project-01/multiagentxr-pipeline.jpg"
link: "https://github.com/RayChen666/Multi-Agent-XR"
---

## Demo Video

<video controls width="100%" style={{ borderRadius: '8px', marginBottom: '2rem' }}>
  <source src="/images/projects/project-01/multiagentxr-Demo-Video.mp4" type="video/mp4" />
  Your browser does not support the video tag.
</video>

## Overview
This is a real-time XR spatial intelligence system that integrates natural language command input 
with a multi-agent backend using FastAPI and LangGraph for agentic collaborative spatial reasoning, 
enabling dynamic, spatially coordinated XR user interactions and manipulations with scene objects, 
demonstrating scalable multi-modal input workflows and immersive interface behaviors.

## Motivation
Creating custom VR scenes inherits the idea of world building and brings it to 3D immersive space 
from traditional 2D environments (like Minecraft). This evolution significantly expands the boundaries 
of Reality Technology and opens new possibilities for human creativity and expression.

## Limitations of existing layout systems

- **Relatively poor spatial reference handling**: current systems struggle in basic user-related commands 
like “place the table next to me”, often misinterpreting directional references and user-relative positioning.
- **Lack of real-time spatial adaptation**: when users make adjustments in the VR environments, existing 
systems cannot dynamically adjust object placement relative to the user's new position, breaking immersion 
and spatial coherence.
- **Limited Multi-scale spatial reasoning**: existing systems cannot handle both scene & room scale spatial 
planning and object-scale positioning in a seemingless workflow.

## Potential Research Questions

- Can multi-agent collaboration produce better spatial layouts than single-agent systems?
- How do specialized agents (parsing, asset, scene, validation) work together to reach coherent arrangement?
- Can agents understand vague prompts from natural language? (eg., “place the table here", 
“make the room layout cozy and more spatial”)

## Pipeline Design

---

### This is an ongoing project, and further updates will follow.



